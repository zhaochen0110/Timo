# **TIMO** üå±
This repo contains the code, data, and models for "[TIMO: Towards Better Temporal Reasoning for Language Models](https://arxiv.org/pdf/2309.05653.pdf)‚Äù.

<div align="center">
 üî• üî• üî• Check out our <a href = "https://tiger-ai-lab.github.io/MAmmoTH/">[Project Page]</a> for more results and analysis!
</div>

### Models
Our models are all available at Huggingface:

- [TIMO-7B](https://huggingface.co/Warrieryes/timo-7b-hf)
- [TIMO-13B](https://huggingface.co/Warrieryes/timo-13b-hf)

## Highlights
We demonstrate the main results of TIMO as follows:

#### 7B Parameter Model

| Model       | Math-time Avg | Pure-time Avg | Average  |
| ----------- | ------------- | ------------- | -------- |
| Timo        | **64.4**      | **78.07**     | **72.7** |
| MAmmoTH     | 57.08         | 62.71         | 60.0     |
| WizardMath  | 58.8          | 61.26         | 59.9     |
| CodeLlama   | 54.55         | 64.10         | 59.8     |
| LLaMA2      | 57.65         | 66.30         | 62.7     |
| WizardCoder | 53.05         | 59.83         | 57.8     |
| ToRA        | 51.03         | 65.71         | 58.2     |
| TimeLLaMA   | 48.3          | 29.0          | 38.6     |

#### 13B Parameter Model

| Model       | Math-time Avg | Pure-time Avg | Average  |
| ----------- | ------------- | ------------- | -------- |
| **Timo**    | **72.83**     | **82.97**     | **78.3** |
| MAmmoTH     | 70.68         | 69.52         | 72.1     |
| LLaMA2      | 66.18         | 70.42         | 70.7     |
| WizardMath  | 63.65         | 70.62         | 68.4     |
| WizardCoder | 61.6          | 66.08         | 65.9     |
| CodeLlama   | 63.55         | 67.05         | 65.7     |
| ToRA        | 57.85         | 68.90         | 65.6     |

## **Table of Contents**

- [üìå Introduction](#introduction)
- [‚öôÔ∏è Installation](#installation)
- [üõ†Ô∏è Training and Inference](#training-and-inference)
- [üìú License](#license)
- [üìñ Citation](#citation)

## üìå **Introduction**
We introduce **TIMO** üå±, a series of open-source large language models (LLMs) designed for temporal reasoning. Through our insightful discoveries of the close relationship between mathematics and temporal reasoning, we introduce a self-critic temporal optimization method to equip the model with comprehensive temporal reasoning capabilities. TIMO models are trained on preference pairs generated by the model itself and critics on temporal tasks, encompassing 19 pure-time temporal tasks. TIMO demonstrates significant generalizability across all temporal tasks without sacrificing its general task abilities, establishing itself as the new state-of-the-art model for comparable sizes.

## ‚öôÔ∏è **Installation**

Clone this repository and install the required packages:

```bash
git clone https://github.com/zhaochen0110/Timo.git
cd Timo
pip install -r requirements.txt
```

## üõ†Ô∏è **Training and Inference**

### **Quick Start**
To play with our model, run:

```python
from transformers import pipeline
pipeline = pipeline("text-generation", "Warrieryes/timo-7b-hf")

alpaca_template = "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{query}\n\n### Response:"

query = "Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?"

### By default, MAmmoTH will output the Chain-of-thought (CoT) rationale
rationale_prefix = ""

### You can let MAmmoTH output Program-of-thought (PoT) rationale by simply adding
rationale_prefix = " Let's write a program."

input = alpaca_template.format(query = query + rationale_prefix)

output = pipeline(input)[0]['generated_text']
print(output)
```

### **Large-scale Evaluation**

To replicate the experimental results in our paper, run:

```bash

```

If you want to run the evaluation for the general task, use the following bash script for [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).

```bash
### 
```

### **Self-critic Temporal Preference Generation**

We use the **[MAmmoTH](https://github.com/TIGER-AI-Lab/MAmmoTH)** project's code to train mathematical models. Then we use the following code to generate Temporal Preference pairs:

```bash
python generate.py \
    --model_path $model_path \
    --generate True \
    --train_data_path $train_data_path \
    --score True \
    --generate_data_path $generate_data_path \
    --save_path $save_path
```

## Temporal direct preference optimization

After generating preference pairs, we use Direct Preference Optimization (DPO) to train the model:
```
deepspeed --include localhost:${DEVICES} --master_port 29502 dpo_train.py \
    --model_name_or_path ${MODELPATH} \
    --json_path ${JSONPATH} \
    --output_dir ${OUTPUTPATH}/${RUNNAME} \
    --num_train_epochs ${DPOEPOCH} \
    --beta 0.1 \
    --per_device_train_batch_size ${BSZPERDEV} \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps ${GRADACC} \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps 10000 \
    --save_total_limit 1 \
    --learning_rate 5e-7 \
    --warmup_ratio 0.1 \
    --lr_scheduler_type "linear" \
    --logging_steps 1 \
    --model_max_length 2048 \
    --report_to "wandb" \
    --run_name ${RUNNAME} \
    --bf16 True \
    --gradient_checkpointing True \
    --deepspeed ./ds_config/stage3_no_offloading_accelerate.json
```

## üìú License

This project is licensed under the Apache 2.0 license - see the LICENSE file for details.

## Acknowledgements

This project is partly based on the work done in **[MAmmoTH](https://github.com/TIGER-AI-Lab/MAmmoTH)**. Special thanks to their authors for valuable contributions.


## **üìñ Citation**

Please cite our paper if you use our data, model or code. Please also kindly cite the original dataset papers. 

```

```
